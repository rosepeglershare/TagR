% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hyperparameter_tuning.R
\name{hyperparameter_tuning}
\alias{hyperparameter_tuning}
\title{hyperparameter_tuning}
\usage{
hyperparameter_tuning(
  train_labelled_dtm,
  valid_labelled_dtm,
  train_labels,
  val_labels,
  topics,
  num_its = 1000
)
}
\arguments{
\item{train_labelled_dtm}{Training labelled document-term matrix.}

\item{valid_labelled_dtm}{Validation labelled document-term matrix.}

\item{train_labels}{Training labels matrix.}

\item{val_labels}{Validation labels matrix.}

\item{topics}{List of topics.}

\item{num_its}{Number of iterations to run for each topic. Default: 1000}
}
\value{
A dataframe with columns representing parameters and rows representing an optimal parameter set for each topic.
}
\description{
Finds the best set of xgboost parameters for each topic using random search.
}
\details{
Parameters:
\itemize{
\item{max_depth: Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.}
\item{eta: Step size shrinkage used in update to prevent overfitting. }
\item{subsample: Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees and this will prevent overfitting. }
\item{colsample_bytree: The subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.}
\item{min_child_weight: Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning.}
}
}
